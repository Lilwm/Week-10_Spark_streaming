# -*- coding: utf-8 -*-
"""Real-Time Network Traffic Analysis.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1rdWiF0We0VqhmIq_72WwRVjWWxZAdLBB

## Prerequisites

Install pyspark and confluent kafka
"""

!pip install pyspark

!pip install confluent-kafka
!pip install kafka-python

from pyspark.sql import SparkSession
from pyspark.sql.functions import window, avg, udf, split, current_timestamp
from pyspark.sql.types import StringType
from confluent_kafka import Producer

# Create SparkSession
spark = SparkSession.builder.appName("NetworkTrafficAnalytics").getOrCreate()

# Set Kafka consumer configuration
kafka_bootstrap_servers = 'pkc-q283m.af-south-1.aws.confluent.cloud:9092'
kafka_security_protocol = 'SASL_SSL'
kafka_sasl_mechanism = 'PLAIN'
kafka_sasl_plain_username = 'WVXAVKT5ZOZYZ2ST'
kafka_sasl_plain_password = 'PIfNcb/jzZoBQbIYnuyduoXDymELg86Nuoj6jsyGlwqzSO7Q5Xqc2R/z+4obL5l/'
kafka_topic = 'network-traffic'

# Create Kafka consumer configuration dictionary
kafka_consumer_config = {
    'bootstrap.servers': kafka_bootstrap_servers,
    'security.protocol': kafka_security_protocol,
    'sasl.mechanism': kafka_sasl_mechanism,
    'sasl.username': kafka_sasl_plain_username,
    'sasl.password': kafka_sasl_plain_password,
    'group.id': 'network-traffic-consumer'
}

# Read data from Kafka topic as a streaming DataFrame
df = spark \
    .readStream \
    .format("kafka") \
    .option("kafka.bootstrap.servers", kafka_bootstrap_servers) \
    .option("kafka.security.protocol", kafka_security_protocol) \
    .option("kafka.sasl.mechanism", kafka_sasl_mechanism) \
    .option("kafka.sasl.username", kafka_sasl_plain_username) \
    .option("kafka.sasl.password", kafka_sasl_plain_password) \
    .option("subscribe", kafka_topic) \
    .option("startingOffsets", "latest") \
    .load()

# Extract the value from the Kafka message
df = df.selectExpr("CAST(value AS STRING)")

# Split the value column into source_ip, destination_ip, and bytes_sent
df = df.withColumn("source_ip", split(df["value"], ",")[0]) \
    .withColumn("destination_ip", split(df["value"], ",")[1]) \
    .withColumn("bytes_sent", split(df["value"], ",")[2].cast("integer"))

# Add a timestamp column to the DataFrame
df = df.withColumn("timestamp", current_timestamp())

# Configure Kafka producer for writing processed data to Kafka topic
kafka_producer_config = {
    'bootstrap.servers': kafka_bootstrap_servers,
    'security.protocol': kafka_security_protocol,
    'sasl.mechanism': kafka_sasl_mechanism,
    'sasl.username': kafka_sasl_plain_username,
    'sasl.password': kafka_sasl_plain_password
}

# Perform analytics using sliding window and window-based aggregation
windowed_df = df \
    .withWatermark("timestamp", "10 seconds") \
    .groupBy(window("timestamp", "10 seconds")) \
    .agg(avg("bytes_sent").alias("average_bytes_sent"))

# Convert window column to string using UDF
window_to_string_udf = udf(lambda window: str(window.start) + ' - ' + str(window.end))
windowed_df = windowed_df.withColumn("window_string", window_to_string_udf("window"))

# Kafka producer delivery report callback
def delivery_report(err, msg):
    if err is not None:
        print(f'Error: Failed to deliver message: {err}')
    else:
        print(f'Message delivered to {msg.topic()} [{msg.partition()}]')

# Write the processed data to Kafka
windowed_df \
    .selectExpr("to_json(struct(*)) AS value", "window_string AS key") \
    .writeStream \
    .foreachBatch(lambda df, epoch_id: df.foreachPartition(write_to_kafka)) \
    .option("checkpointLocation", "/path/to/checkpoint") \
    .start()

# Function to write each partition to Kafka
def write_to_kafka(rows):
    producer = Producer(kafka_producer_config)
    for row in rows:
        key = row.key
        value = row.value
        producer.produce("processed_data", value=value, key=key, callback=delivery_report)
    producer.flush()

# Start the streaming query
spark.streams.awaitAnyTermination()

